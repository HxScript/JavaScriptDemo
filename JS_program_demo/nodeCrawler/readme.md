# nodeJS爬虫：静态内容的爬取想做一个**随机语录**的前端页面，打算自己收集一些句子。可是句子怎么收集呢？之前有一个网站叫一言。但是调用它的api要涉及到跨域的问题，但是我还不擅长解决跨域。再说了，如果我能解决跨域，也就说明我会写后端程序了，那我为什么不自己写一个api呢~而且都放在我的服务器上，这样就不涉及到跨域了。我就想，干脆像十年前的阮一峰老师的[毛主席语录](http://www.ruanyifeng.com/php/mao/)一样，直接写在静态页面里得了。这个当作第一代的版本。下一版本我打算自己在服务器写一个api，能允许前端向同源服务器通过ajax向后端请求到一条语录。而现在我爬虫爬下来的语录，也可以当作未来这个版本后端返回的数据。所以，爬虫就要开始啦。该demo以http://www.juzimi.com/article/大话西游为例。涉及到侵权，联系我，我会立即删除。允许转载，但仅限于学习使用。涉及到其他侵权行为，与hxscript无关。```"use strict" const request = require('request')const cheerio = require('cheerio')function Sentence() {	this.content = ''	this.author = ''	// this.id = 0	this.like = ''}const log = function() {	console.log.apply(console, arguments)}// div 是  .views-field-phpcode// var sen = es('.xqjulistwafo')// var author = es('.xlistju')const senFromDiv = function(div) {	const sentence = new Sentence()	const e = cheerio.load(div)	sentence.content = e('.xqjulistwafo').text()	sentence.author = e('.xlistju').text()	sentence.like = e('.views-field-ops').text()	return sentence}const saveSentence = function(sen) {	const fs = require('fs')	const path = 'sentence.txt'	const s = JSON.stringify(sen, null, 2) + ','	fs.appendFile(path, s, function(error) {		if (error !== null) {            log('*** 写入文件错误', error)        } else {            log('--- 保存成功')        }	})}const sentenceFromUrl = function(url) {	request(url, function(error, response, body) {		if (error === null && response.statusCode == 200) {			const e = cheerio.load(body)			const sen = []			const senDivs = e('.views-field-phpcode')			for(let i = 0; i < senDivs.length; i++) {				let element = senDivs[i]				const div = e(element).html()				const s = senFromDiv(div)				sen.push(s)			}			saveSentence(sen)		} else {			// log('aaaaaaaaaaaaaaaaaaa网站请求错啦！', error)		}	})}const ___main = function() {	for (let i = 0 ;i <= 17; i++) {	const url = 'http://www.juzimi.com/article/%E5%A4%A7%E8%AF%9D%E8%A5%BF%E6%B8%B8?page=' + i	sentenceFromUrl(url)	}}___main()```